{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBmBhOlZckTN"
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, SimpleRNN\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2tRt63ecrOe"
      },
      "source": [
        "date_vars = ['DatetimeBegin','DatetimeEnd']\n",
        "\n",
        "agg_ts = pd.read_csv('BE_1_2013-2015_aggregated_timeseries.csv', sep='\\t', parse_dates=date_vars, date_parser=pd.to_datetime)\n",
        "meta = pd.read_csv('BE_2013-2015_metadata.csv', sep='\\t')\n",
        "\n",
        "print('aggregated timeseries shape:{}'.format(agg_ts.shape))\n",
        "print('metadata shape:{}'.format(meta.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iw766WLcsIq"
      },
      "source": [
        "df = agg_ts.loc[agg_ts.DataAggregationProcess=='P1D', :] \n",
        "df = df.loc[df.UnitOfAirPollutionLevel!='count', :]\n",
        "df = df.loc[df.SamplingPoint.isin(ser_avail_days[ser_avail_days.values >= 1000].index), :]\n",
        "vars_to_drop = ['AirPollutant','AirPollutantCode','Countrycode','Namespace','TimeCoverage','Validity','Verification','AirQualityStation',\n",
        "               'AirQualityStationEoICode','DataAggregationProcess','UnitOfAirPollutionLevel', 'DatetimeEnd', 'AirQualityNetwork',\n",
        "               'DataCapture', 'DataCoverage']\n",
        "df.drop(columns=vars_to_drop, axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0NDQ06pcxfy"
      },
      "source": [
        "dates = list(pd.period_range(min(df.DatetimeBegin), max(df.DatetimeBegin), freq='D').values)\n",
        "samplingpoints = list(df.SamplingPoint.unique())\n",
        "\n",
        "new_idx = []\n",
        "for sp in samplingpoints:\n",
        "    for d in dates:\n",
        "        new_idx.append((sp, np.datetime64(d)))\n",
        "\n",
        "df.set_index(keys=['SamplingPoint', 'DatetimeBegin'], inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "df = df.reindex(new_idx)\n",
        "#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # should contain NaN for the columns\n",
        "\n",
        "df['AirPollutionLevel'] = df.groupby(level=0).AirPollutionLevel.bfill().fillna(0)\n",
        "#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # NaN are replaced by values of 2013-01-30\n",
        "print('{} missing values'.format(df.isnull().sum().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No-w6DIrcx8O"
      },
      "source": [
        "df = df.loc['SPO-BETR223_00001_100',:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3l0zTiPczra"
      },
      "source": [
        "train = df.query('DatetimeBegin < \"2014-07-01\"')\n",
        "valid = df.query('DatetimeBegin >= \"2014-07-01\" and DatetimeBegin < \"2015-01-01\"')\n",
        "test = df.query('DatetimeBegin >= \"2015-01-01\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhkIKf2fc0_6"
      },
      "source": [
        "# Save column names and indices to use when storing as csv\n",
        "cols = train.columns\n",
        "train_idx = train.index\n",
        "valid_idx = valid.index\n",
        "test_idx = test.index\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train = scaler.fit_transform(train)\n",
        "valid = scaler.transform(valid)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNU8XlcGc3jy"
      },
      "source": [
        "train = pd.DataFrame(train, columns=cols, index=train_idx)\n",
        "valid = pd.DataFrame(valid, columns=cols, index=valid_idx)\n",
        "test = pd.DataFrame(test, columns=cols, index=test_idx)\n",
        "\n",
        "train.to_csv('train.csv')\n",
        "valid.to_csv('valid.csv')\n",
        "test.to_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ8gwbl5c496"
      },
      "source": [
        "train = pd.read_csv('train.csv', header=0, index_col=0).values.astype('float32')\n",
        "valid = pd.read_csv('valid.csv', header=0, index_col=0).values.astype('float32')\n",
        "test = pd.read_csv('test.csv', header=0, index_col=0).values.astype('float32')\n",
        "\n",
        "def plot_loss(history, title):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Nb Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    val_loss = history.history['val_loss']\n",
        "    min_idx = np.argmin(val_loss)\n",
        "    min_val_loss = val_loss[min_idx]\n",
        "    print('Minimum validation loss of {} reached at epoch {}'.format(min_val_loss, min_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKzWolUVc6tB"
      },
      "source": [
        "n_lag = 14\n",
        "\n",
        "train_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 5)\n",
        "valid_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)\n",
        "test_data_gen = TimeseriesGenerator(test, test, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn0r8-_lc8Fy"
      },
      "source": [
        "simple_rnn = Sequential()\n",
        "simple_rnn.add(SimpleRNN(4, input_shape=(n_lag, 1)))\n",
        "simple_rnn.add(Dense(1))\n",
        "simple_rnn.compile(loss='mae', optimizer=RMSprop())\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='simple_rnn_weights.hdf5', verbose=0, save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
        "with open(\"simple_rnn.json\", \"w\") as m:\n",
        "    m.write(simple_rnn.to_json())\n",
        "\n",
        "simple_rnn_history = simple_rnn.fit_generator(train_data_gen, epochs=100, validation_data=valid_data_gen, verbose=0\n",
        "                                              , callbacks=[checkpointer, earlystopper])\n",
        "plot_loss(simple_rnn_history, 'SimpleRNN - Train & Validation Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIoocSxic_jb"
      },
      "source": [
        "simple_lstm = Sequential()\n",
        "simple_lstm.add(LSTM(4, input_shape=(n_lag, 1)))\n",
        "simple_lstm.add(Dense(1))\n",
        "simple_lstm.compile(loss='mae', optimizer=RMSprop())\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='simple_lstm_weights.hdf5', verbose=0, save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
        "with open(\"simple_lstm.json\", \"w\") as m:\n",
        "    m.write(simple_lstm.to_json())\n",
        "\n",
        "simple_lstm_history = simple_lstm.fit_generator(train_data_gen, epochs=100, validation_data=valid_data_gen, verbose=0\n",
        "                                                , callbacks=[checkpointer, earlystopper])\n",
        "plot_loss(simple_lstm_history, 'Simple LSTM - Train & Validation Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38gH2_kwdBCE"
      },
      "source": [
        "def eval_best_model(model):\n",
        "    # Load model architecture from JSON\n",
        "    model_architecture = open(model+'.json', 'r')\n",
        "    best_model = model_from_json(model_architecture.read())\n",
        "    model_architecture.close()\n",
        "    # Load best model's weights\n",
        "    best_model.load_weights(model+'_weights.hdf5')\n",
        "    # Compile the best model\n",
        "    best_model.compile(loss='mae', optimizer=RMSprop())\n",
        "    # Evaluate on test data\n",
        "    perf_best_model = best_model.evaluate_generator(test_data_gen)\n",
        "    print('Loss on test data for {} : {}'.format(model, perf_best_model))\n",
        "\n",
        "eval_best_model('simple_rnn')\n",
        "eval_best_model('simple_lstm')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}